{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_fillna(df,feature):\n",
    "    process_df=df.loc[:,feature]\n",
    "    for i in feature:\n",
    "        knonw=process_df[process_df[i].notnull()]\n",
    "        unknown=process_df[process_df[i].isnull()]\n",
    "        if unknown.empty:\n",
    "            continue\n",
    "        else:\n",
    "            y=knonw[i]\n",
    "            x=knonw.drop(i,axis=1)\n",
    "            rfr=RandomForestRegressor(random_state=0,n_estimators=200,max_depth=3,n_jobs=-1)\n",
    "            rfr.fit(x,y)\n",
    "            predict=rfr.predict(unknown.drop(i,axis=1))\n",
    "            process_df.loc[unknown.index,i]=predict\n",
    "    return process_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pca(data,feature):\n",
    "    pca=PCA(n_components=8,random_state=0)\n",
    "    pca.fit(data)\n",
    "    l1=pca.explained_variance\n",
    "    l2=pca.explained_variance_ratio_\n",
    "    l3=pca.components_\n",
    "    data_pca=pca.transform(data)\n",
    "    k1_spss=pca.components_/np.sqrt(pca.explained_variance_.reshape(8,1))\n",
    "    l=[l1,l2,l3,k1_spss]\n",
    "    ratio_sum=l2.sum()\n",
    "    variance_ratio=l2/ratio_sum\n",
    "    list_ratio=np.ones([k1_spss.shape[0],k1_spss.shape[1]])\n",
    "    for i in range(k1_spss.shape[0]):\n",
    "        list_ratio[i]=k1_spss[i]*variance_ratio[i]\n",
    "    feature_weight=np.zeros([k1_spss.shape[0],1])\n",
    "    for i in range(0,k1_spss.shape[1]):\n",
    "        for j in range(0,k1_spss.shape[0]):\n",
    "            feature_weight[j]=feature_weight[j]+list_ratio[j][i]\n",
    "    feature_weight=pd.DataFrame(feature_weight)\n",
    "    feature_weight.index=data.columns\n",
    "    feature_weight.columns=['weight']\n",
    "    return data_pca,feature_weight\n",
    "def get_entropy_weight(data,feature):\n",
    "    data=data.apply(lambda x:(x-x.min())/(x.max()-x.min()))\n",
    "    ent=lambda x:-x*np.log2(x)-(1-x)*np.log2(1-x)\n",
    "    #计算熵\n",
    "    ent=data.apply(lambda x:ent(x)).sum()\n",
    "    #计算信息增益\n",
    "    ent=ent-data.apply(lambda x:x*np.log2(x).sum())\n",
    "    #计算信息增益比\n",
    "    ent=ent/np.log2(len(data.columns))\n",
    "    #计算信息熵权重\n",
    "    ent_weight=1/(1+ent)\n",
    "    return ent_weight\n",
    "def topsis(data,feature_weight):\n",
    "    data=data/np.sqrt((data**2).sum())\n",
    "    #优劣距离\n",
    "    z=pd.DataFrame([data.min(),data.max()],index=['best','worst'])\n",
    "    #距离\n",
    "    result=data.copy()\n",
    "    result['best']=np.sqrt(((data-z.loc['best'])**2).sum(axis=1))\n",
    "    result['worst']=np.sqrt(((data-z.loc['worst'])**2).sum(axis=1))\n",
    "    #综合得分\n",
    "    result['score']=result['worst']/(result['best']+result['worst'])\n",
    "    return result,z,feature_weight\n",
    "def ahp_weight(A_arr):\n",
    "    A=np.array(A_arr)\n",
    "    #按列归一化\n",
    "    B=A/A.sum(axis=0)\n",
    "    print('归一化后的矩阵为：\\n',B)\n",
    "    b_sum=B.sum(axis=1)\n",
    "    print('按行求和后的矩阵为：\\n',b_sum)\n",
    "    #权重\n",
    "    W=b_sum.sum()\n",
    "    w_arr=[]\n",
    "    for w in b_sum:\n",
    "        w_arr.append(w/W)\n",
    "    print('权重为：\\n',w_arr)\n",
    "    AW=[]\n",
    "    for a in A:\n",
    "        AW.append(a*w_arr)\n",
    "    print('矩阵乘以权重后的矩阵为：\\n',AW)\n",
    "    result=np.array(AW)/np.array(w_arr)\n",
    "    print('最终的矩阵为：\\n',result)\n",
    "    row=result.shape[0]\n",
    "    max=result.sum()/row\n",
    "    print('最大特征值为：\\n',max)\n",
    "    CI=(max-row)/(row-1)\n",
    "    print('一致性指标CI为：\\n',CI)\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "def build_spark_client() -> SparkSession:\n",
    "    sc = SparkConf()\n",
    "    spark = SparkSession.builder.config(conf=sc).appName('haozhe_spark').enableHiveSupport().config('hive.exec.dynamic.partition', 'true').config('hive.exec.dynamic.partition.mode', 'nonstrict').config('spark.sql.adaptive.enabled', 'true').config('spark.speculation', 'true').config(\n",
    "        'spark.sql.execution.arrow.pyspark.enable', 'true').config('spark.executor.memory', '20g').config('spark.driver.memory', '20g').config('spark.driver.maxResultSize', '0').config('spark.kryoserializer.buffer.max', '1g').config('spark.sql.source.partitionOverwriteMode', 'dynamic').getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def _map_to_pandas(rdds):\n",
    "    '''needs to be here due to pickling issues'''\n",
    "    return [pd.DataFrame(list(rdds))]\n",
    "def topandas(df,n_partitions=None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if n_partitions is not None:\n",
    "        df=df.repartition(n_partitions)\n",
    "        df_pand=df.rdd.mapPartitions(_map_to_pandas).collect()\n",
    "        df_pand=pd.concat(df_pand)\n",
    "        df_pand.columns=df.columns\n",
    "        return df_pand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=build_spark_client()\n",
    "sql='''\n",
    "select * from haozhe.tianchi_fresh_comp_train_user\n",
    "'''\n",
    "data=spark.sql(sql)\n",
    "data=topandas(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "create table if not exists haozhe.tianchi_fresh_comp_train_user_pca(\n",
    "user_id string,\n",
    "item_id string,\n",
    "behavior_type string\n",
    ")\n",
    "partitioned by (dt string comment '日期')\n",
    "row format serde 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' with serdeproperties ('serialization.format'='1') stored as inputformat 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' outputformat 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' tblproperties ('transient_lastDdlTime'='1659514291')\n",
    "'''\n",
    "spark.sql(sql)\n",
    "sdf=spark.createDataFrame(data)\n",
    "sdf.write.mode('overwrite').insertInto('haozhe.tianchi_fresh_comp_train_user_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4df7a6600e22bae99e6e8f837be5af686fd7a404512ca9c2620376f38fe7d31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
